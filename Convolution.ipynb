{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "互相关运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def corr2d(x, k):\n",
    "    h, w = k.shape\n",
    "    y = torch.zeros((x.shape[0] - h + 1, x.shape[1] - w + 1))\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            y[i, j] = (x[i:i + h, j: j + w] * k).sum() #在torch里面 * 就是互相关运算——即矩阵对应坐标的值相乘\n",
    "\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "卷积层"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros([1,1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = corr2d(x, self.weight) + self.bias\n",
    "        return y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "简单的边缘检测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[1., 1., 0., 0., 0., 0., 1., 1.],\n           [1., 1., 0., 0., 0., 0., 1., 1.],\n           [1., 1., 0., 0., 0., 0., 1., 1.],\n           [1., 1., 0., 0., 0., 0., 1., 1.],\n           [1., 1., 0., 0., 0., 0., 1., 1.],\n           [1., 1., 0., 0., 0., 0., 1., 1.]]]]),\n tensor([[[[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]]]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Conv2d(1, 1, kernel_size = [1, 2], bias = False) #前两个1表示输入输出的通道\n",
    "\n",
    "#形状都是高维度的写在前面的例如二维的[2,1],2是第二行,在张量里面数行就是1维的了,因为如果只有1维的话就代表只有1行,那么既然有2行那就说明是二维的\n",
    "x = torch.ones([1, 1, 6, 8])\n",
    "y = torch.zeros([1, 1, 6, 7])\n",
    "#这里的形状是4维的,第一项代表batch也就是批量大小,在一个迭代中这个数值就代表这是这次迭代中的第几张图片\n",
    "#第二项是通道数,普通的2D彩色图片一般有RGB 3个通道\n",
    "#后面就是一个通道里的大小了\n",
    "x[0, 0, :, 2:6] = 0\n",
    "y[0, 0, :, 1] = 1\n",
    "y[0, 0, :, 5] = -1\n",
    "x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, loss: 28.87433624267578\n",
      "batch 2, loss: 12.239182472229004\n",
      "batch 3, loss: 5.27701473236084\n",
      "batch 4, loss: 2.3303258419036865\n",
      "batch 5, loss: 1.0625720024108887\n",
      "batch 6, loss: 0.5043945908546448\n",
      "batch 7, loss: 0.25086554884910583\n",
      "batch 8, loss: 0.13108451664447784\n",
      "batch 9, loss: 0.07182344049215317\n",
      "batch 10, loss: 0.041022807359695435\n"
     ]
    }
   ],
   "source": [
    "##现在要学一个w出来使他能做边缘检测\n",
    "for i in range(10):\n",
    "    y_hat = net(x)\n",
    "    l = (y_hat - y) ** 2\n",
    "    net.zero_grad() #使网络里所有的参数梯度清零\n",
    "    l.sum().backward()\n",
    "    net.weight.data -= 3e-2 * net.weight.grad\n",
    "\n",
    "    print(f'batch {i+1}, loss: {l.sum()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 0.9641, -1.0012]]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "填充和步长"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 5, 5])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding1_net = nn.Conv2d(1, 1, kernel_size = 3, padding = 1) #kernel_size只写一个标量的话就默认是一个正方形\n",
    "\n",
    "padding1_x = torch.rand([1, 1, 5, 5])\n",
    "\n",
    "padding1_net(padding1_x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 10, 10])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#也可以做高度和宽带不同的填充\n",
    "padding2_net = nn.Conv2d(1, 1, kernel_size = [5, 3], padding = [2, 1])\n",
    "\n",
    "padding2_x = torch.rand([1, 1, 10, 10])\n",
    "\n",
    "padding2_net(padding2_x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 3, 3])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设置步长\n",
    "stride_net = nn.Conv2d(1, 1, kernel_size = 3, padding = 1, stride = 2)\n",
    "\n",
    "stride_x = torch.rand([1, 1, 5, 5])\n",
    "\n",
    "stride_net(stride_x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "多输入通道互相关运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    return sum(d2l.corr2d(x ,k) for x, k in zip(X, K))\n",
    "    #zip表示最大的一个维度,这就就表示对通道做遍历,这样拿出来的就是一个通道的x和k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "多输出通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.Stack(corr2d(X, k) for k in K) #stack可以把一个tensor堆叠,就扩充了维度,在这里就是把不同卷积核的输出stack一下就得到了输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}